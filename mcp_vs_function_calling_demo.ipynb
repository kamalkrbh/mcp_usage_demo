{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9350c55",
   "metadata": {},
   "source": [
    "# MCP Server vs Function Calling Demo\n",
    "\n",
    "**Thesis:** \"MCP server ‚â† Function Calling and is NOT only used in case of AI\"\n",
    "\n",
    "This notebook demonstrates three different approaches:\n",
    "\n",
    "1. **Simple MCP Client** - Non-AI application using MCP protocol\n",
    "2. **AI + MCP Client** - AI application using MCP tools via protocol\n",
    "3. **Traditional Function Calling** - AI application using direct function calls from MCP server\n",
    "\n",
    "## Key Differences:\n",
    "- **MCP Protocol**: Standardized tool discovery and calling over network\n",
    "- **Function Calling**: Direct in-process function execution with AI\n",
    "- **MCP enables non-AI apps** to use the same tools that AI can access!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fbf1bb",
   "metadata": {},
   "source": [
    "## 1. Start MCP Server in Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341570e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "from mcp_server import stop_mcp_server\n",
    "\n",
    "def start_new_mcp_server():\n",
    "    \"\"\"Start a new MCP server\"\"\"\n",
    "    print(\"üöÄ Starting new MCP server...\")\n",
    "    server_process = subprocess.Popen(\n",
    "        [\"python\", \"mcp_server.py\", \"sse\", \"8765\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    # Wait for server to start\n",
    "    print(\"‚è≥ Waiting for server to start...\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Check if server is running\n",
    "    max_retries = 5\n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(\"http://localhost:8765/health\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                print(\"‚úÖ MCP server is running on http://localhost:8765\")\n",
    "                print(f\"üìä Server PID: {server_process.pid}\")\n",
    "                return server_process\n",
    "        except requests.RequestException:\n",
    "            if retry < max_retries - 1:\n",
    "                print(f\"‚è≥ Retry {retry + 1}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Server might still be starting... continuing anyway\")\n",
    "                print(f\"üìä Server PID: {server_process.pid}\")\n",
    "                return server_process\n",
    "    \n",
    "    return server_process\n",
    "\n",
    "# Main execution\n",
    "print(\"üîß MCP SERVER STARTUP PROCESS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Step 1: Stop any existing MCP servers using utility from mcp_server.py\n",
    "stop_mcp_server()\n",
    "\n",
    "# Step 2: Start new MCP server\n",
    "server_process = start_new_mcp_server()\n",
    "\n",
    "print(\"\\n‚úÖ MCP server startup completed!\")\n",
    "print(f\"üåê Server available at: http://localhost:8765\")\n",
    "print(f\"üìä Process ID: {server_process.pid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ac5a1",
   "metadata": {},
   "source": [
    "## 1.1 Stop MCP Server (if needed)\n",
    "\n",
    "Run this cell if you need to stop the MCP server manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp_server import stop_mcp_server\n",
    "\n",
    "# Execute the stop function from mcp_server.py\n",
    "stop_mcp_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d7d8a",
   "metadata": {},
   "source": [
    "## 2. Simple MCP Client (Non-AI Application)\n",
    "\n",
    "This demonstrates that **MCP is NOT just for AI** - any application can use MCP protocol to discover and call tools.\n",
    "\n",
    "### 2.1 Transport Options Example\n",
    "\n",
    "Your MCP server supports different transport methods. Here are the client configurations:\n",
    "\n",
    "**Current Setup (SSE):**\n",
    "```python\n",
    "# Server: python mcp_server.py sse 8765\n",
    "client = Client(\"http://localhost:8765/sse\")\n",
    "```\n",
    "\n",
    "**Alternative Transports:**\n",
    "```python\n",
    "# HTTP: python mcp_server.py http 8765\n",
    "client = Client(\"http://localhost:8765\")\n",
    "\n",
    "# Streamable HTTP: python mcp_server.py streamable-http 8765  \n",
    "client = Client(\"http://localhost:8765/stream\")\n",
    "\n",
    "# STDIO: python mcp_server.py stdio\n",
    "# For STDIO, you can use simple file path syntax:\n",
    "client = Client(\"mcp_server.py\")  # FastMCP automatically handles subprocess\n",
    "\n",
    "# Or explicit subprocess control (for advanced configuration):\n",
    "import subprocess\n",
    "from fastmcp.client.transports import StdioTransport\n",
    "\n",
    "transport = StdioTransport(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_server.py\", \"stdio\"]\n",
    ")\n",
    "client = Client(transport)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a583e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastmcp import Client\n",
    "import json\n",
    "\n",
    "async def simple_mcp_client_demo():\n",
    "    \"\"\"Demonstrate non-AI application using MCP protocol\"\"\"\n",
    "    print(\"üîç SIMPLE MCP CLIENT DEMO (Non-AI Application)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # MCP Client URL Options based on server transport:\n",
    "    # - SSE (current):           \"http://localhost:8765/sse\"\n",
    "    # - HTTP:                    \"http://localhost:8765\"\n",
    "    # - Streamable HTTP:         \"http://localhost:8765/stream\"\n",
    "    # - STDIO:                   Client(\"mcp_server.py\") - simple file path syntax\n",
    "    client = Client(\"http://localhost:8765/sse\")\n",
    "    async with client:\n",
    "        # 1. Server Health Check\n",
    "        print(\"\\nüîó SERVER CONNECTIVITY:\")\n",
    "        print(\"=\" * 25)\n",
    "        try:\n",
    "            ping_result = await client.ping()\n",
    "            print(\"‚úÖ Server ping successful!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Server ping failed: {e}\")\n",
    "        \n",
    "        # 2. Server Information\n",
    "        print(\"\\nüìä SERVER INFORMATION:\")\n",
    "        print(\"=\" * 25)\n",
    "        try:\n",
    "            server_info = await client.get_server_info()\n",
    "            print(\"‚úÖ Server Info:\")\n",
    "            print(json.dumps(server_info, indent=2))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Server info not available: {e}\")\n",
    "        \n",
    "        try:\n",
    "            capabilities = await client.get_server_capabilities()\n",
    "            print(\"\\n‚úÖ Server Capabilities:\")\n",
    "            print(json.dumps(capabilities, indent=2))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Server capabilities not available: {e}\")\n",
    "        \n",
    "        # 3. Complete Discovery via MCP Protocol\n",
    "        print(\"\\nüìã DISCOVERY - ALL AVAILABLE FEATURES:\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        # Tools discovery\n",
    "        print(\"\\nüõ†Ô∏è Available Tools:\")\n",
    "        tools = await client.list_tools()\n",
    "        print(f\"Found {len(tools)} tools:\")\n",
    "        for tool in tools:\n",
    "            print(f\"  ‚Ä¢ {tool.name}: {tool.description}\")\n",
    "        \n",
    "        # Resources discovery\n",
    "        print(\"\\nüìÇ Available Resources:\")\n",
    "        try:\n",
    "            resources = await client.list_resources()\n",
    "            if resources:\n",
    "                print(f\"Found {len(resources)} resources:\")\n",
    "                for resource in resources:\n",
    "                    print(f\"  ‚Ä¢ {resource.uri}: {resource.name}\")\n",
    "            else:\n",
    "                print(\"  No resources available\")\n",
    "        except Exception as e:\n",
    "            print(f\"  No resources available: {e}\")\n",
    "        \n",
    "        # Prompts discovery\n",
    "        print(\"\\nüìù Available Prompts:\")\n",
    "        try:\n",
    "            prompts = await client.list_prompts()\n",
    "            if prompts:\n",
    "                print(f\"Found {len(prompts)} prompts:\")\n",
    "                for prompt in prompts:\n",
    "                    print(f\"  ‚Ä¢ {prompt.name}: {prompt.description}\")\n",
    "            else:\n",
    "                print(\"  No prompts available\")\n",
    "        except Exception as e:\n",
    "            print(f\"  No prompts available: {e}\")\n",
    "        \n",
    "        # 4. Show MCP Tool Syntax and Parameters\n",
    "        print(\"\\nüîß MCP TOOL SYNTAX & PARAMETERS:\")\n",
    "        print(\"=\" * 40)\n",
    "        for tool in tools:\n",
    "            print(f\"\\nüìÑ Tool: {tool.name}\")\n",
    "            print(f\"   Description: {tool.description}\")\n",
    "            if hasattr(tool, 'inputSchema') and tool.inputSchema:\n",
    "                print(f\"   Parameters Schema:\")\n",
    "                print(json.dumps(tool.inputSchema, indent=4))\n",
    "            else:\n",
    "                print(\"   Parameters: No schema available\")\n",
    "        \n",
    "        # 5. Execute tools via MCP Protocol\n",
    "        print(\"\\nüõ†Ô∏è  EXECUTING MCP TOOLS:\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        print(\"\\nüå§Ô∏è  Calling get_weather tool...\")\n",
    "        weather_result = await client.call_tool(\"get_weather\", {\"city\": \"London\"})\n",
    "        print(f\"Result: {weather_result.structured_content}\")\n",
    "        \n",
    "        print(\"\\nüßÆ Calling calculate tool...\")\n",
    "        calc_result = await client.call_tool(\"calculate\", {\"operation\": \"multiply\", \"a\": 15, \"b\": 4})\n",
    "        print(f\"Result: {calc_result.structured_content}\")\n",
    "        \n",
    "        print(\"\\nüë§ Calling get_user_info tool...\")\n",
    "        user_result = await client.call_tool(\"get_user_info\", {\"user_id\": 1})\n",
    "        print(f\"Result: {user_result.structured_content}\")\n",
    "        \n",
    "        # 6. Advanced MCP Operations\n",
    "        print(\"\\nüî¨ ADVANCED MCP OPERATIONS:\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        # Resource operations (if available)\n",
    "        print(\"\\nüìÇ Resource Operations:\")\n",
    "        try:\n",
    "            # Try to read a resource\n",
    "            resource_content = await client.read_resource(\"example://resource/1\")\n",
    "            print(f\"   Resource content: {resource_content}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   No resources to read (expected for this server): {e}\")\n",
    "        \n",
    "        # Prompt operations (if available)\n",
    "        print(\"\\nüìù Prompt Operations:\")\n",
    "        try:\n",
    "            # Try to get a prompt\n",
    "            prompt_result = await client.get_prompt(\"welcome\", {\"name\": \"User\"})\n",
    "            print(f\"   Prompt result: {prompt_result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   No prompts available (expected for this server): {e}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Simple MCP client demo completed!\")\n",
    "        print(\"üí° Key insight: Non-AI applications can use MCP protocol for comprehensive server interaction\")\n",
    "        print(\"üìù Notice: MCP protocol provides automatic discovery of tools, resources, and prompts!\")\n",
    "        print(\"üîß Demonstrated methods: ping(), get_server_info(), get_server_capabilities(),\")\n",
    "        print(\"   list_tools(), list_resources(), list_prompts(), call_tool(), read_resource(), get_prompt()\")\n",
    "\n",
    "# Handle event loop for notebook environment\n",
    "try:\n",
    "    await simple_mcp_client_demo()\n",
    "except RuntimeError:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    await simple_mcp_client_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd9b77",
   "metadata": {},
   "source": [
    "## 3. AI + MCP Client (Using Groq LLaMA)\n",
    "\n",
    "This shows AI using MCP protocol to access tools - demonstrating AI + MCP integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from fastmcp import Client\n",
    "import json\n",
    "\n",
    "# Set up Groq API key (you'll need to set this)\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY') or \"your-groq-api-key-here\"\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "async def ai_mcp_client_demo():\n",
    "    \"\"\"Demonstrate AI application using MCP protocol with actual LLM calls\"\"\"\n",
    "    print(\"ü§ñ AI + MCP CLIENT DEMO (Real LLM Integration)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Connect to MCP server\n",
    "    # MCP Client URL Options based on server transport:\n",
    "    # - SSE (current):           \"http://localhost:8765/sse\"\n",
    "    # - HTTP:                    \"http://localhost:8765\"\n",
    "    # - Streamable HTTP:         \"http://localhost:8765/stream\"\n",
    "    # - STDIO:                   Client(\"mcp_server.py\") - simple file path syntax\n",
    "    mcp_client = Client(\"http://localhost:8765/sse\")\n",
    "    async with mcp_client:\n",
    "        # Get available tools from MCP server\n",
    "        print(\"\\nüìã AI discovering tools via MCP...\")\n",
    "        tools = await mcp_client.list_tools()\n",
    "        available_tools = [tool.name for tool in tools]\n",
    "        print(f\"Available tools: {available_tools}\")\n",
    "        \n",
    "        # Show tools with their MCP schemas\n",
    "        print(\"\\nüîç AI analyzing MCP tool schemas:\")\n",
    "        tools_description = []\n",
    "        for tool in tools:\n",
    "            print(f\"\\n‚Ä¢ {tool.name}:\")\n",
    "            print(f\"  Description: {tool.description}\")\n",
    "            \n",
    "            tool_info = f\"- {tool.name}: {tool.description}\"\n",
    "            if hasattr(tool, 'inputSchema') and tool.inputSchema:\n",
    "                properties = tool.inputSchema.get('properties', {})\n",
    "                required = tool.inputSchema.get('required', [])\n",
    "                print(f\"  Parameters: {list(properties.keys())}\")\n",
    "                print(f\"  Required: {required}\")\n",
    "                \n",
    "                params_info = []\n",
    "                for param, details in properties.items():\n",
    "                    param_type = details.get('type', 'unknown')\n",
    "                    is_required = \" (required)\" if param in required else \" (optional)\"\n",
    "                    params_info.append(f\"{param}: {param_type}{is_required}\")\n",
    "                tool_info += f\" Parameters: {', '.join(params_info)}\"\n",
    "            else:\n",
    "                print(\"  Parameters: No schema\")\n",
    "                tool_info += \" Parameters: None\"\n",
    "            \n",
    "            tools_description.append(tool_info)\n",
    "        \n",
    "        # User requests to process\n",
    "        user_requests = [\n",
    "            \"What's the weather like in Tokyo?\",\n",
    "            \"Calculate 25 divided by 5\",\n",
    "            \"Get information for user ID 2\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nüéØ AI processing user requests using LLM + MCP tools:\")\n",
    "        \n",
    "        for user_input in user_requests:\n",
    "            print(f\"\\nüë§ User: {user_input}\")\n",
    "            \n",
    "            # Create prompt for LLM to choose the right tool\n",
    "            tools_list = \"\\n\".join(tools_description)\n",
    "            prompt = f\"\"\"You are an AI assistant that can call tools to help users. \n",
    "                            Available tools:\n",
    "                            {tools_list}\n",
    "\n",
    "                            User query: \"{user_input}\"\n",
    "\n",
    "                            Based on the user query, determine which tool to use and what parameters to provide.\n",
    "                            Respond with ONLY a JSON object in this format:\n",
    "                            {{\n",
    "                                \"tool_name\": \"tool_name_here\",\n",
    "                                \"parameters\": {{\"param1\": \"value1\", \"param2\": \"value2\"}}\n",
    "                            }}\n",
    "\n",
    "                            Do not include any other text in your response.\"\"\"\n",
    "\n",
    "            try:\n",
    "                # Call LLM to decide which tool to use\n",
    "                print(\"üß† Asking LLM to choose the right tool...\")\n",
    "                \n",
    "                if GROQ_API_KEY != \"your-groq-api-key-here\":\n",
    "                    chat_completion = groq_client.chat.completions.create(\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        model=\"llama3-8b-8192\",\n",
    "                        temperature=0.1\n",
    "                    )\n",
    "                    \n",
    "                    llm_response = chat_completion.choices[0].message.content.strip()\n",
    "                    print(f\"üß† LLM response: {llm_response}\")\n",
    "                    \n",
    "                    # Parse LLM response\n",
    "                    try:\n",
    "                        tool_choice = json.loads(llm_response)\n",
    "                        tool_name = tool_choice[\"tool_name\"]\n",
    "                        parameters = tool_choice[\"parameters\"]\n",
    "                        \n",
    "                        print(f\"ü§ñ AI selected: {tool_name} with parameters {parameters}\")\n",
    "                        \n",
    "                        # Call the chosen tool via MCP\n",
    "                        result = await mcp_client.call_tool(tool_name, parameters)\n",
    "                        print(f\"   Result: {result.structured_content}\")\n",
    "                        \n",
    "                    except json.JSONDecodeError:\n",
    "                        print(\"‚ùå Failed to parse LLM response as JSON\")\n",
    "                else:\n",
    "                    # Fallback simulation when no API key\n",
    "                    print(\"‚ö†Ô∏è  No Groq API key - simulating LLM response...\")\n",
    "                    if \"weather\" in user_input.lower():\n",
    "                        tool_choice = {\"tool_name\": \"get_weather\", \"parameters\": {\"city\": \"Tokyo\"}}\n",
    "                    elif \"calculate\" in user_input.lower() or \"divided\" in user_input.lower():\n",
    "                        tool_choice = {\"tool_name\": \"calculate\", \"parameters\": {\"operation\": \"divide\", \"a\": 25, \"b\": 5}}\n",
    "                    elif \"user\" in user_input.lower():\n",
    "                        tool_choice = {\"tool_name\": \"get_user_info\", \"parameters\": {\"user_id\": 2}}\n",
    "                    \n",
    "                    print(f\"ü§ñ Simulated AI choice: {tool_choice['tool_name']} with parameters {tool_choice['parameters']}\")\n",
    "                    result = await mcp_client.call_tool(tool_choice[\"tool_name\"], tool_choice[\"parameters\"])\n",
    "                    print(f\"   Result: {result.structured_content}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error in LLM call or tool execution: {e}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ AI + MCP demo completed!\")\n",
    "        print(\"üí° Key insight: LLM chooses tools dynamically based on user query and MCP tool schemas\")\n",
    "        print(\"üìù Notice: Same tools discovered via MCP, but LLM makes the intelligent choice!\")\n",
    "\n",
    "# Handle event loop for notebook environment\n",
    "try:\n",
    "    await ai_mcp_client_demo()\n",
    "except RuntimeError:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "    await ai_mcp_client_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a50017",
   "metadata": {},
   "source": [
    "## 4. Traditional Function Calling with AI\n",
    "\n",
    "This demonstrates traditional function calling where we import the actual functions from the MCP server and use them directly (without the @mcp.tool decorator effect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af36e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import inspect\n",
    "from mcp_server import get_weather, calculate, get_user_info\n",
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "# Groq client for LLM calls\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY') or \"your-groq-api-key-here\"\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "def create_function_schema_from_mcp_tools():\n",
    "    \"\"\"Create function calling schemas dynamically from MCP server tools\"\"\"\n",
    "    # Import the actual functions from mcp_server (same functions, used directly for function calling)\n",
    "    mcp_functions = {\n",
    "        \"get_weather\": get_weather,\n",
    "        \"calculate\": calculate, \n",
    "        \"get_user_info\": get_user_info\n",
    "    }\n",
    "    \n",
    "    schemas = []\n",
    "    for func_name, func in mcp_functions.items():\n",
    "        # Get function signature and docstring\n",
    "        sig = inspect.signature(func)\n",
    "        doc = func.__doc__ or f\"Function {func_name}\"\n",
    "        \n",
    "        # Build parameters schema from function signature\n",
    "        properties = {}\n",
    "        required = []\n",
    "        \n",
    "        for param_name, param in sig.parameters.items():\n",
    "            param_type = \"string\"  # default\n",
    "            if param.annotation == int:\n",
    "                param_type = \"integer\"\n",
    "            elif param.annotation == float:\n",
    "                param_type = \"number\"\n",
    "            elif param.annotation == str:\n",
    "                param_type = \"string\"\n",
    "            \n",
    "            properties[param_name] = {\n",
    "                \"type\": param_type,\n",
    "                \"description\": f\"The {param_name} parameter\"\n",
    "            }\n",
    "            \n",
    "            if param.default == inspect.Parameter.empty:\n",
    "                required.append(param_name)\n",
    "        \n",
    "        schema = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": func_name,\n",
    "                \"description\": doc.strip(),\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\", \n",
    "                    \"properties\": properties,\n",
    "                    \"required\": required\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        schemas.append(schema)\n",
    "    \n",
    "    return schemas, mcp_functions\n",
    "\n",
    "def function_calling_demo():\n",
    "    \"\"\"Demonstrate traditional function calling with AI using actual MCP server functions and real LLM\"\"\"\n",
    "    print(\"‚ö° TRADITIONAL FUNCTION CALLING DEMO (Real LLM Integration)\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    # Generate schemas from actual MCP server functions\n",
    "    function_schemas, available_functions = create_function_schema_from_mcp_tools()\n",
    "    \n",
    "    print(f\"\\nüìã Using actual functions from mcp_server.py:\")\n",
    "    for func_name in available_functions.keys():\n",
    "        print(f\"  ‚Ä¢ {func_name}\")\n",
    "    \n",
    "    print(f\"\\nüîß Generated {len(function_schemas)} function schemas dynamically\")\n",
    "    print(\"üìù Schemas created from function signatures & docstrings\")\n",
    "    \n",
    "    # Show function calling schemas in detail\n",
    "    print(f\"\\nüìÑ FUNCTION CALLING SCHEMAS:\")\n",
    "    print(\"=\" * 35)\n",
    "    for i, schema in enumerate(function_schemas, 1):\n",
    "        print(f\"\\n{i}. Function: {schema['function']['name']}\")\n",
    "        print(f\"   Description: {schema['function']['description']}\")\n",
    "        print(f\"   Parameters:\")\n",
    "        properties = schema['function']['parameters']['properties']\n",
    "        required = schema['function']['parameters']['required']\n",
    "        for param_name, param_info in properties.items():\n",
    "            req_mark = \" (required)\" if param_name in required else \" (optional)\"\n",
    "            print(f\"     ‚Ä¢ {param_name}: {param_info['type']}{req_mark}\")\n",
    "            print(f\"       {param_info['description']}\")\n",
    "    \n",
    "    # Show one complete example schema\n",
    "    print(f\"\\nüìã Complete JSON Schema Example (get_weather):\")\n",
    "    print(json.dumps(function_schemas[0], indent=2))\n",
    "    \n",
    "    # Simulate user requests with LLM integration\n",
    "    user_requests = [\n",
    "        \"What's the weather in New York?\",\n",
    "        \"Add 10 and 15\", \n",
    "        \"Get user 3's information\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüéØ EXECUTING FUNCTION CALLS WITH LLM:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for user_input in user_requests:\n",
    "        print(f\"\\nüë§ User: {user_input}\")\n",
    "        \n",
    "        # Create prompt for LLM with function schemas\n",
    "        functions_description = []\n",
    "        for schema in function_schemas:\n",
    "            func_info = schema['function']\n",
    "            name = func_info['name']\n",
    "            desc = func_info['description']\n",
    "            params = func_info['parameters']['properties']\n",
    "            required = func_info['parameters']['required']\n",
    "            \n",
    "            param_list = []\n",
    "            for param, details in params.items():\n",
    "                param_type = details['type']\n",
    "                is_req = \" (required)\" if param in required else \" (optional)\"\n",
    "                param_list.append(f\"{param}: {param_type}{is_req}\")\n",
    "            \n",
    "            functions_description.append(f\"- {name}: {desc} | Parameters: {', '.join(param_list)}\")\n",
    "        \n",
    "        functions_list = \"\\n\".join(functions_description)\n",
    "        \n",
    "        prompt = f\"\"\"You are an AI assistant that can call functions to help users.\n",
    "Available functions:\n",
    "{functions_list}\n",
    "\n",
    "User query: \"{user_input}\"\n",
    "\n",
    "Based on the user query, determine which function to call and what parameters to provide.\n",
    "Respond with ONLY a JSON object in this format:\n",
    "{{\n",
    "    \"function_name\": \"function_name_here\",\n",
    "    \"parameters\": {{\"param1\": \"value1\", \"param2\": \"value2\"}}\n",
    "}}\n",
    "\n",
    "Do not include any other text in your response.\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Call LLM to decide which function to use\n",
    "            print(\"üß† Asking LLM to choose the right function...\")\n",
    "            \n",
    "            if GROQ_API_KEY != \"your-groq-api-key-here\":\n",
    "                chat_completion = groq_client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    model=\"llama3-8b-8192\",\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                \n",
    "                llm_response = chat_completion.choices[0].message.content.strip()\n",
    "                print(f\"üß† LLM response: {llm_response}\")\n",
    "                \n",
    "                # Parse LLM response\n",
    "                try:\n",
    "                    function_choice = json.loads(llm_response)\n",
    "                    function_name = function_choice[\"function_name\"]\n",
    "                    parameters = function_choice[\"parameters\"]\n",
    "                    \n",
    "                    print(f\"ü§ñ AI selected: {function_name} with parameters {parameters}\")\n",
    "                    \n",
    "                    # Call the chosen function directly\n",
    "                    if function_name in available_functions:\n",
    "                        if function_name == \"get_weather\":\n",
    "                            result = available_functions[function_name](parameters[\"city\"])\n",
    "                        elif function_name == \"calculate\":\n",
    "                            result = available_functions[function_name](\n",
    "                                parameters[\"operation\"], \n",
    "                                parameters[\"a\"], \n",
    "                                parameters[\"b\"]\n",
    "                            )\n",
    "                        elif function_name == \"get_user_info\":\n",
    "                            result = available_functions[function_name](parameters[\"user_id\"])\n",
    "                        \n",
    "                        print(f\"   Result: {result}\")\n",
    "                    else:\n",
    "                        print(f\"‚ùå Unknown function: {function_name}\")\n",
    "                        \n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"‚ùå Failed to parse LLM response as JSON\")\n",
    "            else:\n",
    "                # Fallback simulation when no API key\n",
    "                print(\"‚ö†Ô∏è  No Groq API key - simulating LLM response...\")\n",
    "                if \"weather\" in user_input.lower():\n",
    "                    city = \"New York\" if \"new york\" in user_input.lower() else \"London\"\n",
    "                    function_choice = {\"function_name\": \"get_weather\", \"parameters\": {\"city\": city}}\n",
    "                elif \"add\" in user_input.lower():\n",
    "                    function_choice = {\"function_name\": \"calculate\", \"parameters\": {\"operation\": \"add\", \"a\": 10, \"b\": 15}}\n",
    "                elif \"user\" in user_input.lower():\n",
    "                    function_choice = {\"function_name\": \"get_user_info\", \"parameters\": {\"user_id\": 3}}\n",
    "                \n",
    "                print(f\"ü§ñ Simulated AI choice: {function_choice['function_name']} with parameters {function_choice['parameters']}\")\n",
    "                \n",
    "                # Execute the simulated choice\n",
    "                if function_choice[\"function_name\"] == \"get_weather\":\n",
    "                    result = available_functions[\"get_weather\"](function_choice[\"parameters\"][\"city\"])\n",
    "                elif function_choice[\"function_name\"] == \"calculate\":\n",
    "                    result = available_functions[\"calculate\"](\n",
    "                        function_choice[\"parameters\"][\"operation\"],\n",
    "                        function_choice[\"parameters\"][\"a\"],\n",
    "                        function_choice[\"parameters\"][\"b\"]\n",
    "                    )\n",
    "                elif function_choice[\"function_name\"] == \"get_user_info\":\n",
    "                    result = available_functions[\"get_user_info\"](function_choice[\"parameters\"][\"user_id\"])\n",
    "                \n",
    "                print(f\"   Result: {result}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in LLM call or function execution: {e}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Function calling demo completed!\")\n",
    "    print(\"üí° Key insight: LLM chooses functions based on user query and function schemas\")\n",
    "    print(\"üìù Notice: Same functions from MCP server, but called directly with LLM guidance\")\n",
    "    print(\"üîÑ LLM analyzes schemas ‚Üí chooses function ‚Üí app executes direct call\")\n",
    "\n",
    "# Run the demo\n",
    "function_calling_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc879bdd",
   "metadata": {},
   "source": [
    "## 5. Comparison Summary\n",
    "\n",
    "Let's compare the three approaches we just demonstrated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1472cac",
   "metadata": {},
   "source": [
    "## 5. Comparison Summary\n",
    "\n",
    "### Let's compare the three approaches we just demonstrated:\n",
    "\n",
    "| Aspect               | MCP Server                   | Function Calling            |\n",
    "|----------------------|------------------------------|-----------------------------|\n",
    "| Protocol             | Standardized MCP protocol    | AI-specific implementation  |\n",
    "| Communication        | Network-based (HTTP/SSE/WS)  | In-process direct calls     |\n",
    "| Tool Discovery       | Dynamic via list_tools()     | Static JSON schema          |\n",
    "| Schema Discovery     | Automatic via MCP protocol   | Manual introspection/coding |\n",
    "| LLM Integration      | LLM + MCP tools list         | LLM + function schemas      |\n",
    "| Decision Making      | LLM chooses from MCP tools   | LLM chooses from func schemas|\n",
    "| Language Support     | Language agnostic            | Language dependent          |\n",
    "| Scalability          | Multi-client, distributed    | Single application          |\n",
    "| Coupling             | Loose - tools independent    | Tight - embedded functions  |\n",
    "| Reusability          | High - any client can use    | Low - app-specific          |\n",
    "| Use Cases            | Universal tool sharing       | AI-specific functionality   |\n",
    "| Maintenance          | Centralized tool updates     | Per-app function updates    |\n",
    "| Performance          | Network overhead             | Direct function calls       |\n",
    "| Parameter Info       | Rich schema via protocol     | Function signatures only    |\n",
    "\n",
    "\n",
    "### üéØ KEY INSIGHTS\n",
    "\n",
    "#### üèÜ MCP Advantages:\n",
    "- Tool sharing across applications\n",
    "- Standardized protocol with rich schemas\n",
    "- Language-agnostic integration\n",
    "- Network-based scalability\n",
    "- NOT limited to AI use cases\n",
    "- Dynamic tool discovery with full parameter info\n",
    "- Automatic schema generation and validation\n",
    "- LLM gets tools list from MCP server dynamically\n",
    "\n",
    "#### üèÜ Function Calling Advantages:\n",
    "- Lower latency (no network)\n",
    "- Simpler implementation\n",
    "- Better for single-app scenarios\n",
    "- Direct function execution\n",
    "- No network dependencies\n",
    "- LLM works with static function schemas\n",
    "\n",
    "### üß† LLM INTEGRATION COMPARISON\n",
    "\n",
    "**üîÑ MCP Flow:**\n",
    "1. User query ‚Üí App\n",
    "2. App gets MCP tools list\n",
    "3. User query + MCP tools ‚Üí LLM\n",
    "4. LLM chooses tool + parameters\n",
    "5. App calls MCP tool\n",
    "6. Result returned to user\n",
    "\n",
    "**üîÑ Function Calling Flow:**\n",
    "1. User query ‚Üí App\n",
    "2. App has predefined function schemas\n",
    "3. User query + function schemas ‚Üí LLM\n",
    "4. LLM chooses function + parameters\n",
    "5. App calls function directly\n",
    "6. Result returned to user\n",
    "\n",
    "### üåü CONCLUSION\n",
    "Both approaches use LLM for intelligent tool/function selection, but MCP provides dynamic tool discovery and universal sharing while Function Calling offers direct execution with lower latency.\n",
    "\n",
    "**This demo showed:**\n",
    "- SAME functions used in 3 different ways\n",
    "- LLM decides which tool/function to use in both approaches\n",
    "- MCP provides rich tool schemas automatically\n",
    "- Non-AI applications can discover and use MCP tools\n",
    "- AI applications can use MCP tools with LLM guidance\n",
    "- Traditional function calling uses LLM with static schemas\n",
    "- MCP enables universal tool sharing with dynamic discovery\n",
    "\n",
    "**üîç Schema & LLM Integration:**\n",
    "- MCP: LLM analyzes dynamic tool list from server\n",
    "- Function Calling: LLM analyzes static function schemas\n",
    "- MCP: Rich parameter descriptions and validation\n",
    "- Function Calling: Basic type information from introspection\n",
    "- Both: LLM makes intelligent choices based on user intent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afefee58",
   "metadata": {},
   "source": [
    "## 6. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the MCP server\n",
    "print(\"üßπ CLEANING UP DEMO ENVIRONMENT\")\n",
    "print(\"=\" * 35)\n",
    "print(\"üõë Stopping MCP server...\")\n",
    "server_process.terminate()\n",
    "server_process.wait()\n",
    "print(\"‚úÖ MCP server stopped\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup completed!\")\n",
    "print(\"\\nüìù Demo Summary:\")\n",
    "print(\"  1. ‚úÖ Started MCP server\")\n",
    "print(\"  2. ‚úÖ Demonstrated simple MCP client (non-AI)\")\n",
    "print(\"  3. ‚úÖ Showed AI + MCP integration\")\n",
    "print(\"  4. ‚úÖ Compared with function calling using SAME functions\")\n",
    "print(\"  5. ‚úÖ Generated schemas dynamically from MCP server functions\")\n",
    "print(\"  6. ‚úÖ Cleaned up resources\")\n",
    "\n",
    "print(\"\\nüéâ Demo completed successfully!\")\n",
    "print(\"\\nüéØ Proven: MCP ‚â† Function Calling\")\n",
    "print(\"üí° MCP enables universal tool sharing beyond AI!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
